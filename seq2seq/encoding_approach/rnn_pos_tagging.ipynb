{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 설계\n",
    "1. 데이터 준비\n",
    "    - 개념 설명을 위한 하드코딩된 예시 샘플\n",
    "    - 간단한 문장 및 그에 대한 품사 태그 정의\n",
    "2. 어휘 사전(Vocabulary) 및 태그 사전(Tab Vocabulary) 구성\n",
    "    - 토큰 -> 숫자 변환을 위한 word2index, index2word 매핑 생성\n",
    "    - 품사 태그 - 숫자 매핑(e.g, tag2index, index2tag)\n",
    "3. Pytorch Dataset & DataLoader 정의\n",
    "    - Dataset 클래스 활용, 샘플(토큰 시퀀스) 및 정답(품사 태그 시퀀스) 반환\n",
    "    - DataLoader 활용, 배치(batch) 단위 데이터 처리\n",
    "4. 모델(Encoder-Only RNN) 설계\n",
    "    - Embedding Layer: 입력 단어 임베딩 벡터 변환\n",
    "    - RNN: 시퀀스 정보 인코딩\n",
    "    - Fully-Connected Layer: RNN의 은닉 상태(hidden state) 출력을 받아 각 시점(time-step)마다 품사 태그 예측\n",
    "    - 특징: 일반적인 seq2seq 모델(Encoder-Decoder)와 달리, Encoding-only 접근에서는 인코더에서 바로 분류 수행\n",
    "5. 학습(Training) 루프\n",
    "    - 손실 함수(loss function): 토큰 단위 교차 엔토로피 손실\n",
    "    - 옵티마이저(optimizer): Adam, SGD 등\n",
    "    - Epoch마다 전체 데이터 반복, 예측-손실 계산-역전파-가중치 갱신 수행\n",
    "6. 추론(Inference) 및 성능 확인\n",
    "    - 학습된 모델의 샘플 데이터 예측 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 코드 구현 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 호출\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 예시 데이터 준비\n",
    "sentences = [\n",
    "    [\"나는\", \"밥을\", \"먹는다\"],\n",
    "    [\"너는\", \"공부를\", \"한다\"],\n",
    "    [\"그녀는\", \"책을\", \"읽는다\"],\n",
    "]\n",
    "\n",
    "pos_tags = [\n",
    "    [\"PRON\", \"NOUN\", \"VERB\"],\n",
    "    [\"PRON\", \"NOUN\", \"VERB\"],\n",
    "    [\"PRON\", \"NOUN\", \"VERB\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vocabulary: {'너는': 1, '책을': 2, '공부를': 3, '나는': 4, '그녀는': 5, '읽는다': 6, '밥을': 7, '한다': 8, '먹는다': 9}\n",
      "Tag Vocabulary: {'PRON': 0, 'NOUN': 1, 'VERB': 2}\n"
     ]
    }
   ],
   "source": [
    "# 2. Vocabulary 생성\n",
    "word_list = set([word for sent in sentences for word in sent])\n",
    "word_list = list(word_list)\n",
    "word2index = {w: i+1 for i, w in enumerate(word_list)}\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "\n",
    "tag_list = set([tag for tags in pos_tags for tag in tags])\n",
    "tag_list = list(tag_list)\n",
    "tag2index = {t: i for i, t in enumerate(tag_list)}\n",
    "index2tag = {i: t for t, i in tag2index.items()}\n",
    "\n",
    "vocab_size = len(word2index) + 1\n",
    "tag_size = len(tag2index)\n",
    "\n",
    "print(\"Word Vocabulary:\", word2index)\n",
    "print(\"Tag Vocabulary:\", tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset 및 DataLoader 생성\n",
    "def encode_sentence(sentence):\n",
    "    \"\"\"\n",
    "    단어 사전 활용, 문장 -> 숫자 리스트 변환\n",
    "    예: [\"나는\", \"밥을\", \"먹는다\"] -> [2, 5, 1]\n",
    "    \"\"\"\n",
    "    return [word2index[w] for w in sentence]\n",
    "\n",
    "def encode_tag(tags):\n",
    "    \"\"\"\n",
    "    태그 사전 활용, 태그 -> 숫자 리스트 변환\n",
    "    예: [\"PRON\", \"NOUN\", \"VERB\"] -> [1, 0, 2]\n",
    "    \"\"\"\n",
    "    return [tag2index[t] for t in tags]\n",
    "\n",
    "class PosDataset(Dataset):\n",
    "    def __init__(self, sentences, pos_tags):\n",
    "        super(PosDataset, self).__init__()\n",
    "        self.sentences = sentences\n",
    "        self.pos_tags = pos_tags\n",
    "\n",
    "        # 미리 숫자 인코딩\n",
    "        self.encoded_sentences = [encode_sentence(sent) for sent in sentences]\n",
    "        self.encoded_tags = [encode_tag(tags) for tags in pos_tags]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 각 샘플을 (입력 시퀀스, 타깃 시퀀스) 형태 변환\n",
    "        return (\n",
    "            torch.tensor(self.encoded_sentences[idx], dtype=torch.long),\n",
    "            torch.tensor(self.encoded_tags[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "dataset = PosDataset(sentences, pos_tags)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 모델(Encoder-Only RNN) 정의\n",
    "class PosTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN(LSTM) 기반의 품사 태깅 모델.\n",
    "    단순화를 위해 bidirectionl=False, single-layer로 구성\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tag_size):\n",
    "        super(PosTagger, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tag_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len] 형태의 단어 인덱스 시퀀스\n",
    "        \"\"\"\n",
    "        # 1) 임베딩\n",
    "        embedded = self.embedding(x) # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # 2) LSTM 적용\n",
    "        #    output: [batch_size, seq_len, hidden_dim]\n",
    "        #    (h_n, c_n): LSTM의 마지막 히든, 셀 상태(여기서는 사용 X)\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "\n",
    "        # 시점별 fc 적용, 태그 확률(로짓) 산출\n",
    "        logits = self.fc(output) # [batch_size, seq_len, tag_size]\n",
    "\n",
    "        return logits\n",
    "    \n",
    "embedding_dim = 8\n",
    "hidden_dim = 8\n",
    "\n",
    "model = PosTagger(vocab_size, embedding_dim, hidden_dim, tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6460\n",
      "Epoch [2/10], Loss: 1.5302\n",
      "Epoch [3/10], Loss: 1.4318\n",
      "Epoch [4/10], Loss: 1.3755\n",
      "Epoch [5/10], Loss: 1.2593\n",
      "Epoch [6/10], Loss: 1.1556\n",
      "Epoch [7/10], Loss: 1.0485\n",
      "Epoch [8/10], Loss: 0.9408\n",
      "Epoch [9/10], Loss: 0.8350\n",
      "Epoch [10/10], Loss: 0.7382\n",
      "Epoch [11/10], Loss: 0.6478\n",
      "Epoch [12/10], Loss: 0.5868\n",
      "Epoch [13/10], Loss: 0.5215\n",
      "Epoch [14/10], Loss: 0.4565\n",
      "Epoch [15/10], Loss: 0.3947\n",
      "Epoch [16/10], Loss: 0.3535\n",
      "Epoch [17/10], Loss: 0.2988\n",
      "Epoch [18/10], Loss: 0.2751\n",
      "Epoch [19/10], Loss: 0.2291\n",
      "Epoch [20/10], Loss: 0.2165\n",
      "Epoch [21/10], Loss: 0.1789\n",
      "Epoch [22/10], Loss: 0.1729\n",
      "Epoch [23/10], Loss: 0.1423\n",
      "Epoch [24/10], Loss: 0.1402\n",
      "Epoch [25/10], Loss: 0.1270\n",
      "Epoch [26/10], Loss: 0.1068\n",
      "Epoch [27/10], Loss: 0.0978\n",
      "Epoch [28/10], Loss: 0.0960\n",
      "Epoch [29/10], Loss: 0.0832\n",
      "Epoch [30/10], Loss: 0.0756\n",
      "Epoch [31/10], Loss: 0.0702\n",
      "Epoch [32/10], Loss: 0.0672\n",
      "Epoch [33/10], Loss: 0.0631\n",
      "Epoch [34/10], Loss: 0.0614\n",
      "Epoch [35/10], Loss: 0.0544\n",
      "Epoch [36/10], Loss: 0.0515\n",
      "Epoch [37/10], Loss: 0.0507\n",
      "Epoch [38/10], Loss: 0.0493\n",
      "Epoch [39/10], Loss: 0.0470\n",
      "Epoch [40/10], Loss: 0.0442\n",
      "Epoch [41/10], Loss: 0.0405\n",
      "Epoch [42/10], Loss: 0.0412\n",
      "Epoch [43/10], Loss: 0.0396\n",
      "Epoch [44/10], Loss: 0.0359\n",
      "Epoch [45/10], Loss: 0.0367\n",
      "Epoch [46/10], Loss: 0.0355\n",
      "Epoch [47/10], Loss: 0.0323\n",
      "Epoch [48/10], Loss: 0.0312\n",
      "Epoch [49/10], Loss: 0.0322\n",
      "Epoch [50/10], Loss: 0.0312\n",
      "Epoch [51/10], Loss: 0.0304\n",
      "Epoch [52/10], Loss: 0.0294\n",
      "Epoch [53/10], Loss: 0.0267\n",
      "Epoch [54/10], Loss: 0.0280\n",
      "Epoch [55/10], Loss: 0.0253\n",
      "Epoch [56/10], Loss: 0.0246\n",
      "Epoch [57/10], Loss: 0.0258\n",
      "Epoch [58/10], Loss: 0.0234\n",
      "Epoch [59/10], Loss: 0.0228\n",
      "Epoch [60/10], Loss: 0.0222\n",
      "Epoch [61/10], Loss: 0.0234\n",
      "Epoch [62/10], Loss: 0.0212\n",
      "Epoch [63/10], Loss: 0.0207\n",
      "Epoch [64/10], Loss: 0.0222\n",
      "Epoch [65/10], Loss: 0.0198\n",
      "Epoch [66/10], Loss: 0.0211\n",
      "Epoch [67/10], Loss: 0.0206\n",
      "Epoch [68/10], Loss: 0.0202\n",
      "Epoch [69/10], Loss: 0.0201\n",
      "Epoch [70/10], Loss: 0.0178\n",
      "Epoch [71/10], Loss: 0.0191\n",
      "Epoch [72/10], Loss: 0.0187\n",
      "Epoch [73/10], Loss: 0.0167\n",
      "Epoch [74/10], Loss: 0.0164\n",
      "Epoch [75/10], Loss: 0.0161\n",
      "Epoch [76/10], Loss: 0.0158\n",
      "Epoch [77/10], Loss: 0.0173\n",
      "Epoch [78/10], Loss: 0.0170\n",
      "Epoch [79/10], Loss: 0.0165\n",
      "Epoch [80/10], Loss: 0.0162\n",
      "Epoch [81/10], Loss: 0.0160\n",
      "Epoch [82/10], Loss: 0.0159\n",
      "Epoch [83/10], Loss: 0.0140\n",
      "Epoch [84/10], Loss: 0.0137\n",
      "Epoch [85/10], Loss: 0.0150\n",
      "Epoch [86/10], Loss: 0.0147\n",
      "Epoch [87/10], Loss: 0.0145\n",
      "Epoch [88/10], Loss: 0.0143\n",
      "Epoch [89/10], Loss: 0.0126\n",
      "Epoch [90/10], Loss: 0.0138\n",
      "Epoch [91/10], Loss: 0.0136\n",
      "Epoch [92/10], Loss: 0.0121\n",
      "Epoch [93/10], Loss: 0.0132\n",
      "Epoch [94/10], Loss: 0.0133\n",
      "Epoch [95/10], Loss: 0.0128\n",
      "Epoch [96/10], Loss: 0.0114\n",
      "Epoch [97/10], Loss: 0.0125\n",
      "Epoch [98/10], Loss: 0.0110\n",
      "Epoch [99/10], Loss: 0.0123\n",
      "Epoch [100/10], Loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "# 6. 학습 루프\n",
    "for epoch in range(100):\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        # inputs: [batch_size, seq_len]\n",
    "        # targets: [batch_size, seq_len]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) # [batch_size, seq_len, tag_size]\n",
    "\n",
    "        batch_size, seq_len, _ = outputs.shape\n",
    "        outputs_reshaped = outputs.view(batch_size * seq_len, -1)\n",
    "        targets_reshaped = targets.view(-1)\n",
    "\n",
    "        loss = criterion(outputs_reshaped, targets_reshaped)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sentence: ['그녀는', '밥을', '먹는다']\n",
      "Predicted POS: ['PRON', 'NOUN', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "def predict_tags(model, sentences):\n",
    "    \"\"\"\n",
    "    주어진 문장(단어 리스트)에 대해 품사 태그 예측 반환\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded = torch.tensor([word2index[word] for word in sentences], dtype=torch.long)\n",
    "        # [1, seq_len]\n",
    "\n",
    "        encoded = encoded.unsqueeze(0) # [1, seq_len]\n",
    "\n",
    "        logits = model(encoded) # [1, seq_len, tag_size]\n",
    "        predictions = torch.argmax(logits, dim=-1) # [1, seq_len]\n",
    "\n",
    "        pred_tags = [index2tag[idx.item()] for idx in predictions[0]]\n",
    "        return pred_tags\n",
    "    \n",
    "test_sentence = [\"그녀는\", \"밥을\", \"먹는다\"]\n",
    "predicted = predict_tags(model, test_sentence)\n",
    "print(\"Test Sentence:\", test_sentence)\n",
    "print(\"Predicted POS:\", predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
